<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hard Soft Attention &mdash; sketchbook  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=92ae9c4e" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            sketchbook
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Machine Learning:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_discriminant_analysis.html">Bayes Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="logistic_regression.html">Logistic Regression</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">sketchbook</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Hard Soft Attention</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/ml/attention.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Attention is a cognitive and behavioural function that enables us to focus on a
subset of the incoming data or information. This function allows us to ignore
irrelevant perceptible information and allows us to concentrate on high-value
information.</p>
<p>Attention mechanisms can be categorized into two classes:</p>
<ul class="simple">
<li><p>Bottom-up unconscious attention: Saliency-based attention are stimulated by
external factors. As an example, louder voices are heard more easily compared
to quieter ones. (Max-pooling and gating mechanisms)</p></li>
<li><p>Top-Down conscious attention: Focused attention has a predetermined goal and
follows specific tasks.</p></li>
</ul>
<p>One way to visualize implicit attention is by looking at the partial derivatives
with respect to the input (a.k.a. the Jacobian matrix).</p>
<p>We may have reasons to explicitly enforce implicit attention, we can achieve
this by placing more weight on sensitive parts of the model.</p>
<section id="hard-soft-attention">
<h1>Hard Soft Attention<a class="headerlink" href="#hard-soft-attention" title="Link to this heading">ÔÉÅ</a></h1>
<p>Hard / Soft attention: Hard attention means the function are described by
discrete variables, while soft attention is described by continuous variables.
This distinction is important because the derivatives of these functions are
either step or smooth. Hard attention is non-diffrential, therefore we cannot
use gradient descent techniques to train. This is is why Reinforcement Learning
(RL) techniques such as policy gradients and REINFORCE algorithm is commonly
used.</p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Chris Choi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>