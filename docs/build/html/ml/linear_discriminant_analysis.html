<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bayes Classifier &mdash; sketchbook  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=73cce014" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Logistic Regression" href="logistic_regression.html" />
    <link rel="prev" title="Linear Regression" href="linear_regression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            sketchbook
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Machine Learning:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayes Classifier</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear-discriminant-analysis-for-p-1">Linear Discriminant Analysis for <span class="math notranslate nohighlight">\(p = 1\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Linear Discriminant Analysis for <span class="math notranslate nohighlight">\(p &gt; 1\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="logistic_regression.html">Logistic Regression</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">sketchbook</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Bayes Classifier</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/ml/linear_discriminant_analysis.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bayes-classifier">
<h1>Bayes Classifier<a class="headerlink" href="#bayes-classifier" title="Link to this heading"></a></h1>
<p>Recall the Bayes’ Theorem:</p>
<div class="math notranslate nohighlight" id="equation-eq-bayes-classifier">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-bayes-classifier" title="Link to this equation"></a></span>\[\begin{split}P(Y = k | X = x)
  &amp;= \dfrac{P(X | Y = k) \; P(Y = k)}{P(X = x)} \\
  &amp;= \dfrac{P(X | Y = k) \; P(Y = k)}
      {\sum_{i}^{K} P(X | Y = i) P(Y = i)} \\
  &amp;= \dfrac{f_k(x) \; \pi_k}
      {\sum_{i}^{K} f_i(x) \; \pi_i}\end{split}\]</div>
<p>where the left hand side, <span class="math notranslate nohighlight">\(P(Y = k | X = x)\)</span>, is called the posterior
probability and gives the probability that an observation <span class="math notranslate nohighlight">\(X = x\)</span> belongs
to the <span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> class. Let <span class="math notranslate nohighlight">\(f_k(X) \equiv P(X | Y = k)\)</span> denote
the density function of <span class="math notranslate nohighlight">\(X\)</span> for an observation that comes from the
<span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> class, and <span class="math notranslate nohighlight">\(\pi_k \equiv P(Y = k)\)</span> represent the
prior probability that a randomly chosen observation comes from the
<span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> class.</p>
<p>Rather than attempting to compute the posterior probability <span class="math notranslate nohighlight">\(P(Y = k | X
= x)\)</span>, we can plug in estimates of <span class="math notranslate nohighlight">\(f_k(x)\)</span> and <span class="math notranslate nohighlight">\(\pi_k\)</span>. The
challenge is estimating the density function <span class="math notranslate nohighlight">\(f_k(x)\)</span>. Different
classifiers use different estimates of <span class="math notranslate nohighlight">\(f_k(x)\)</span> to approximate the Bayes
classifier.</p>
<section id="linear-discriminant-analysis-lda">
<h2>Linear Discriminant Analysis (LDA)<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Link to this heading"></a></h2>
<p>Linear Discriminant Analysis (LDA) is a classifier with a linear decision
boundary, it does so by fitting class conditional densities to the data using
Bayes’ rule.</p>
<p>LDA assumes:</p>
<ul class="simple">
<li><p>Each class is Gaussian with a mean <span class="math notranslate nohighlight">\(\mu_k\)</span>, and variance
<span class="math notranslate nohighlight">\(\sigma^{2}_{k}\)</span> for the <span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> class</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-normal-dist">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-normal-dist" title="Link to this equation"></a></span>\[f_k(x) = \dfrac{1}{\sqrt{2\pi} \; \sigma_k}
  \exp \left( -\dfrac{1}{2\sigma^{2}_{k}} (x - \mu_k)^{2} \right)\]</div>
<ul class="simple">
<li><p>All classes share the variance</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-equal-var">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-equal-var" title="Link to this equation"></a></span>\[\sigma^{2}_{1} = \sigma^{2}_{2} = \cdots = \sigma^{2}_{K}\]</div>
<section id="linear-discriminant-analysis-for-p-1">
<h3>Linear Discriminant Analysis for <span class="math notranslate nohighlight">\(p = 1\)</span><a class="headerlink" href="#linear-discriminant-analysis-for-p-1" title="Link to this heading"></a></h3>
<p>Assume we have only one predictor, <span class="math notranslate nohighlight">\(p = 1\)</span>. Substituting
the normal density function <a class="reference internal" href="#equation-eq-normal-dist">(2)</a> and shared variance <a class="reference internal" href="#equation-eq-equal-var">(3)</a> into the Bayes classifier <a class="reference internal" href="#equation-eq-bayes-classifier">(1)</a>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(Y = k | X = x)
  &amp;= \dfrac{f_k(x) \; \pi_k}
      {\sum_{i}^{K} f_i(x) \; \pi_i}
  \\
  &amp;= \dfrac{
    \pi_k \;
    \frac{1}{\sqrt{2\pi} \; \sigma}
      \exp \left( -\frac{1}{2\sigma^{2}} (x - \mu_k)^{2} \right)
  }
  {
    \sum_{i}^{K}
    \pi_i \;
    \frac{1}{\sqrt{2\pi} \; \sigma}
      \exp \left( -\frac{1}{2\sigma^{2}} (x - \mu_i)^{2} \right)
  }\end{split}\]</div>
<p>taking the log on both sides and rearranging the terms,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\log P(Y = k | X = x)
  &amp;=
    \log ( \pi_k )
    - \log ( \sqrt{2\pi} \; \sigma )
    -\frac{1}{2\sigma^{2}} (x - \mu_k)^{2}
    + \text{const} \\
  &amp;=
    \log ( \pi_k )
    -\frac{1}{2\sigma^{2}} (x - \mu_k)^{2}
    + \text{const} \\
  &amp;=
    \log ( \pi_k )
    -\frac{x^2}{{2\sigma^{2}}}
    -\frac{2 x \mu_k}{{2\sigma^{2}}}
    +\frac{\mu_k^2}{{2\sigma^{2}}}
    + \text{const} \\
  &amp;=
    \log ( \pi_k )
    -\frac{x \mu_k}{{\sigma^{2}}}
    +\frac{\mu_k^2}{{2\sigma^{2}}}
    + \text{const} \\\end{split}\]</div>
<p>we arrive at the Linear discriminant function <span class="math notranslate nohighlight">\(\delta_k(x)\)</span></p>
<div class="math notranslate nohighlight" id="equation-ldf">
<span class="eqno">(4)<a class="headerlink" href="#equation-ldf" title="Link to this equation"></a></span>\[\delta_k(x) =
  \log ( \pi_k )
  -\frac{x \mu_k}{{\sigma^{2}}}
  +\frac{\mu_k^2}{{2\sigma^{2}}}\]</div>
<p>The Bayes decision boundary is the point for which <span class="math notranslate nohighlight">\(\delta_1(x) = \delta_2(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[x = \dfrac{\mu^{2}_{1} - \mu^{2}_{2}}{2 (\mu_{1} - \mu_{2})}
  = \dfrac{\mu_{1} + \mu_{2}}{2}\]</div>
<p>The Linear Discriminant analysis (LDA) method approximates the Bayes classifier
by plugging in estimates for <span class="math notranslate nohighlight">\(\pi_k\)</span>, <span class="math notranslate nohighlight">\(\mu_k\)</span> and
<span class="math notranslate nohighlight">\(\sigma^{2}\)</span> into the Linear discriminant function in <a class="reference internal" href="#equation-ldf">(4)</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\mu}_k &amp;= \dfrac{1}{n_k} \sum_{i:y_i = k} x_i \\
\hat{\sigma}^{2} &amp;= \dfrac{1}{n - K}
  \sum_{k=1}^{K} \sum_{i:y_i=k} (\mu_i - \hat{\mu}_k)^2\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the total number of observations, <span class="math notranslate nohighlight">\(n_k\)</span> is the number
of observations in the <span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> class. The estimate for
<span class="math notranslate nohighlight">\(\mu_k\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span> are simply the mean and variance of
all observations of the <span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> class.</p>
</section>
<section id="id1">
<h3>Linear Discriminant Analysis for <span class="math notranslate nohighlight">\(p &gt; 1\)</span><a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>In the multiple predictor case, we assume that observations <span class="math notranslate nohighlight">\(\mathbf{X} =
(\mathbf{X}_1 \mathbf{X}_2, \cdots, \mathbf{X}_p)\)</span> is drawn from multi-variate
Gaussian distribution, with a class-specific mean vector and common covariance
matrix. Formally, the multivariate Gaussian density is defined as:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{(2\pi)^{\frac{p}{2}} \; \|\Sigma_k\|^{\frac{1}{2}}}
  \exp
  \left(
    - \frac{1}{2}
    (x - \mu)^{T}
    \Sigma^{-1}
    (x - \mu)
  \right)\]</div>
<p>In the case of <span class="math notranslate nohighlight">\(p &gt; 1\)</span> predictors, the Linear discriminant function is:</p>
<div class="math notranslate nohighlight" id="equation-ldf-multi">
<span class="eqno">(5)<a class="headerlink" href="#equation-ldf-multi" title="Link to this equation"></a></span>\[\delta_k(x) =
  \log ( \pi_k )
  + \mathbf{x}^{T} \boldsymbol{\Sigma^{-1}} \mathbf{\mu}_k
  + \dfrac{1}{2} \mathbf{\mu}_{k}^{T} \boldsymbol{\Sigma^{-1}} \mathbf{\mu}_{k}\]</div>
<p>this is the matrix / vector version of <span class="xref std std-ref">ldf</span>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="linear_regression.html" class="btn btn-neutral float-left" title="Linear Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="logistic_regression.html" class="btn btn-neutral float-right" title="Logistic Regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Chris Choi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>